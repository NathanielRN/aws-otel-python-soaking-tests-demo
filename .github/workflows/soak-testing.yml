name: Soak Testing
on:
  # schedule:
  #   - cron: '0 10 * * *' # every day at 10 am UTC: pst 2am
  push:
    branches: [ main ]
env:
  # Language Specific
  PROCESS_COMMAND_LINE: /usr/local/bin/python3 application.py
  PROCESS_EXECUTABLE_NAME: python3
  # Global Env Vars
  AWS_REGION: us-west-2
  MINUTES_TO_RUN: 15
  # Add this?
  # RATE: TPS
  HOSTMETRICS_COLLECTION_INTERVAL_IN_SECONDS: 600

jobs:
  test_apps_and_publish_results:
    name: Publish App and Soak Test and Publish Results
    runs-on: ubuntu-latest
    permissions:
      contents: write
      packages: write
    strategy:
      fail-fast: false
      matrix:
        app-platform: [ flask ]
        # instrumentation-type: [ auto, manual ]
        instrumentation-type: [ auto ]
    env:
      APP_PATH: integration-test-apps/${{ matrix.instrumentation-type}}-instrumentation/${{ matrix.app-platform }}
      IMAGE_SUFFIX: sample-app-${{ matrix.app-platform }}-${{ matrix.instrumentation-type }}-${{ github.sha }}
    steps:
      - uses: actions/checkout@v2
      - uses: actions/checkout@v2
        if: ${{ matrix.instrumentation-type }} == manual
        with:
          repository: open-telemetry/opentelemetry-python
          path: ${{ env.APP_PATH }}/opentelemetry-python-core
      - uses: actions/checkout@v2
        if: ${{ matrix.instrumentation-type }} == manual
        with:
          repository: open-telemetry/opentelemetry-python-contrib
          path: ${{ env.APP_PATH }}/opentelemetry-python-contrib
      - name: Log in to the GitHub Container Registry
        uses: docker/login-action@v1
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v1
      - name: Cache Docker layers
        uses: actions/cache@v2
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-
      - name: Extract metadata (tags, labels) for Docker
        id: docker-image-metadata
        uses: docker/metadata-action@98669ae865ea3cffbcbaa878cf57c20bbf1c6c38
        with:
          images: ghcr.io/${{ github.repository }}-${{ env.IMAGE_SUFFIX }}
      - name: Build and Push Docker image
        uses: docker/build-push-action@v2
        with:
          push: true
          context: ${{ env.APP_PATH }}
          tags: ${{ steps.docker-image-metadata.outputs.tags }}
          labels: ${{ steps.docker-image-metadata.outputs.labels }}
          cache-from: type=local,src=/tmp/.buildx-cache
          cache-to: type=local,dest=/tmp/.buildx-cache
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      - name: Run Sample App & OTel Collector + Load Generator
        working-directory: .github/app-collector-combo
        env:
          APP_IMAGE: ${{ steps.docker-image-metadata.outputs.tags }}
          INSTANCE_ID: ${{ github.run_id }}-${{ github.run_number }}
          LISTEN_ADDRESS: 0.0.0.0:8080
          LOG_GROUP_NAME: ${{ github.repository }}/soak-tests
          LOGS_NAMESPACE: ${{ github.repository }}/soak-tests
          LOG_STREAM_NAME: ${{ env.IMAGE_SUFFIX }}
          PROCESS_COMMAND_LINE: ${{ env.PROCESS_COMMAND_LINE }}
          PROCESS_EXECUTABLE_NAME: ${{ env.PROCESS_EXECUTABLE_NAME }}
          HOSTMETRICS_COLLECTION_INTERVAL_IN_SECONDS: ${{ env.HOSTMETRICS_COLLECTION_INTERVAL_IN_SECONDS }}s
          SOAK_TEST_DURATION: ${{ env.MINUTES_TO_RUN }}m
        run: docker-compose up -d
      - name: Poll alarms during Soak Test
        continue-on-error: true
        id: check-failure-during-soak-tests
        run: >-
          pip install psutil;
          POLL_ALARM_EXIT_CODE=$(
            python3 ./.github/scripts/poll-during-soak-tests.py --polling-interval ${{ env.HOSTMETRICS_COLLECTION_INTERVAL_IN_SECONDS }}
          );
          echo "::set-output name=POLL_ALARM_EXIT_CODE::$POLL_ALARM_EXIT_CODE"
          exit $POLL_ALARM_EXIT_CODE
      - name: Check if polling Soak Test alarms failed to start correctly
        if: ${{ steps.check-failure-during-soak-tests.outputs.POLL_ALARM_EXIT_CODE }} == 1
        run: exit 1
      - name: Get number of CPUs
        run: echo "::set-output name=NUM_OF_CPUS::$(nproc --all)"
        id: get-num-of-cpus-step
      - name: Get a snapshot of metrics
        run: >-
          mkdir -p soak-tests/snapshots/${{ github.sha }};
          for metric_kind in cpu-load memory-usage;
          do
            aws cloudwatch get-metric-widget-image --metric-widget "$(
              cat ./.github/aws-cloudwatch-api-values/get-metric-widget-statistics/$metric_kind-metric-widget.json |
              sed -e "s/<MINUTES_TO_RUN>/${{ env.MINUTES_TO_RUN }}/g" \
                  -e "s/<METRIC_PERIOD>/${{ env.HOSTMETRICS_COLLECTION_INTERVAL_IN_SECONDS }}/g" \
                  -e "s/<NUM_OF_CPUS>/${{ steps.get-num-of-cpus-step.outputs.NUM_OF_CPUS }}/g" \
                  -e "s/<LOGS_NAMESPACE>/$(echo ${{ github.repository }}/soak-tests | sed 's/\//\\\//g')/g" \
                  -e "s/<DIMENSION_1_VALUE>/$(echo ${{ env.PROCESS_COMMAND_LINE }} | sed 's/\//\\\//g')/g"
            )" |
            jq -r '.MetricWidgetImage' |
            base64 -d > soak-tests/snapshots/${{ github.sha }}/${{ matrix.app-platform }}-${{ matrix.instrumentation-type }}-$metric_kind-results.png;
          done
      - name: Commit snapshots to repository
        uses: EndBug/add-and-commit@v7
        with:
          add: soak-tests/snapshots
          branch: gh-pages
          message: 'Adding Soak Tests Snapshots'
      - name: Move back to the main branch
        run: git checkout main
      - name: Prepare Soak Test results as JSON output
        run: >-
          declare -a COMMON_API_PARAMETERS=(
          --namespace ${{ github.repository }}/soak-tests
          --start-time $(date -u -d '${{ env.MINUTES_TO_RUN }} minutes ago' +%FT%TZ)
          --end-time $(date -u +%FT%TZ)
          --period ${{ env.HOSTMETRICS_COLLECTION_INTERVAL_IN_SECONDS }}
          --region ${{ env.AWS_REGION }}
          --statistics Sum
          --dimensions Name=process.command_line,Value='${{ env.PROCESS_COMMAND_LINE }}'
          );
          AVERAGE_CPU_LOAD=$(
            aws cloudwatch get-metric-statistics "${COMMON_API_PARAMETERS[@]}" --metric-name process.cpu.time |
            jq -r '{
              Name: "Soak Test Average CPU Load",
              Value: [.Datapoints[].Sum] | (add / length / ${{ env.HOSTMETRICS_COLLECTION_INTERVAL_IN_SECONDS }} / ${{ steps.get-num-of-cpus-step.outputs.NUM_OF_CPUS }} * 100),
              Unit: "Percent"
            }'
          )
          AVERAGE_VIRTUAL_MEMORY=$(
            aws cloudwatch get-metric-statistics "${COMMON_API_PARAMETERS[@]}" --metric-name process.memory.virtual_usage |
            jq -r "{
              Name: \"Soak Test Average Virtual Memory Used\",
              Value: [.Datapoints[].Sum] | (add / length / $(echo '2^20' | bc)),
              Unit: \"Megabytes\"
            }"
          );
          jq -n "[$AVERAGE_CPU_LOAD] + [$AVERAGE_VIRTUAL_MEMORY] | {benchmarks: .}" |
          tee output.json
      - name: Report on Soak Test Averages results
        uses: NathanielRN/github-action-benchmark@v1.8.2-alpha3
        continue-on-error: true
        id: check-failure-after-soak-tests
        with:
          name: Soak Test Results - sample-app-${{ matrix.app-platform }}-${{ matrix.instrumentation-type }}
          tool: custombenchmark
          output-file-path: output.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          max-items-in-chart: 100
          alert-threshold: 1%
          # Does not work as expected, see: https://github.com/open-telemetry/opentelemetry-python/pull/1478
          # comment-always: true
          fail-on-alert: true
          auto-push: ${{ github.ref == 'refs/heads/main' }}
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: soak-tests/per-commit-overall-results
      - name: Publish Issue if failed during Soak Tests
        uses: JasonEtco/create-an-issue@v2
        if: steps.check-failure-after-soak-tests.outcome == 'failure'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          filename: .github/issue-templates/FAILURE-DURING-SOAK-TESTS.md
          update_existing: true
      - name: Publish Issue if failed after Soak Tests
        uses: JasonEtco/create-an-issue@v2
        if: steps.check-failure-after-soak-tests.outcome == 'failure'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          filename: .github/issue-templates/FAILURE-AFTER-SOAK-TESTS.md
          update_existing: true
      - name: Determine if should fail the workflow
        if: ${{ failure() }}
        run: >-
          echo 'Soak Tests failed, see logs for details';
          exit 1;
