name: Soak Testing
on:
  # schedule:
  #   - cron: '0 10 * * *' # every day at 10 am UTC: pst 2am
  push:
    branches: [ main ]
env:
  # Language Specific
  PROCESS_COMMAND_LINE: /usr/local/bin/python3 application.py
  PROCESS_EXECUTABLE_NAME: python3
  # Global Env Vars
  AWS_REGION: us-west-2
  SOAK_TEST_DURATION_IN_MINUTES: 1
  # Add this?
  # RATE: TPS
  HOSTMETRICS_COLLECTION_INTERVAL_IN_SECONDS: 10

jobs:
  test_apps_and_publish_results:
    name: Publish app and Soak Test - (${{ matrix.app-platform }}, ${{ matrix.instrumentation-type }})
    runs-on: ubuntu-latest
    permissions:
      contents: write # TODO: Do I need this?
      packages: write
      issues: write
    strategy:
      fail-fast: false
      matrix:
        app-platform: [ flask ]
        # instrumentation-type: [ auto, manual ]
        instrumentation-type: [ auto ]
    env:
      IMAGE_SUFFIX: sample-app-${{ matrix.app-platform }}-${{ matrix.instrumentation-type }}-${{ github.sha }}
    steps:
      - uses: actions/checkout@v2
      - name: Log in to the GitHub Container Registry
        uses: docker/login-action@v1
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v1
      - name: Cache Docker layers
        uses: actions/cache@v2
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-
      - name: Extract metadata (tags, labels) for Docker
        id: docker-image-metadata
        uses: docker/metadata-action@98669ae865ea3cffbcbaa878cf57c20bbf1c6c38
        with:
          images: ghcr.io/${{ github.repository }}-${{ env.IMAGE_SUFFIX }}
      - name: Build and Push Docker image
        uses: docker/build-push-action@v2
        with:
          push: true
          context: integration-test-apps/${{ matrix.instrumentation-type}}-instrumentation/${{ matrix.app-platform }}
          tags: ${{ steps.docker-image-metadata.outputs.tags }}
          labels: ${{ steps.docker-image-metadata.outputs.labels }}
          cache-from: type=local,src=/tmp/.buildx-cache
          cache-to: type=local,dest=/tmp/.buildx-cache
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      - name: Get number of CPUs
        run: echo "::set-output name=NUM_OF_CPUS::$(nproc --all)"
        id: get-num-of-cpus-step
      - name: Prepare Metric Data Queries JSON Inputs
        id: prepared-metric-data-queries-json-inputs
        run: >-
          for metric_kind in cpu-load total-memory;
          do
            echo "::set-output name=$metric_kind::$(
                cat ./.github/aws-cloudwatch-api-values/get-metric-data/$metric_kind-metric-data-query.json |
                sed -e "s/<METRIC_PERIOD>/${{ env.HOSTMETRICS_COLLECTION_INTERVAL_IN_SECONDS }}/g" \
                    -e "s/<NUM_OF_CPUS>/${{ steps.get-num-of-cpus-step.outputs.NUM_OF_CPUS }}/g" \
                    -e "s/<LOGS_NAMESPACE>/$(echo ${{ github.repository }}/soak-tests | sed 's/\//\\\//g')/g" \
                    -e "s/<PROCESS_COMMAND_LINE_DIMENSION_VALUE>/$(echo ${{ env.PROCESS_COMMAND_LINE }} | sed 's/\//\\\//g')/g"
              )";
          done
      - name: Create Soak Test alarms
        run: >-
          declare -a COMMON_API_PARAMETERS=(
          --evaluation-periods 1
          --datapoints-to-alarm 1
          --comparison-operator GreaterThanOrEqualToThreshold
          --treat-missing-data breaching
          );
          aws cloudwatch put-metric-alarm "${COMMON_API_PARAMETERS[@]}" \
            --alarm-name 'OTel Python Soak Tests - CPU Load Percentage Spike'
            --alarm-description 'Triggers when the CPU Load Percentage spikes above the allowed threshold DURING the Soak Test.' \
            --threshold 69.5
            --cli-input-json "{ \"Metrics\": ${{ steps.prepared-metric-data-queries-json-inputs.outputs.cpu-load }} }"
          aws cloudwatch put-metric-alarm "${COMMON_API_PARAMETERS[@]}" \
            --alarm-name 'OTel Python Soak Tests - Virtual Memory Usage Spike'
            --alarm-description 'Triggers when the Virtual Memory Usage spikes above the allowed threshold DURING the Soak Test.' \
            --threshold "$(echo '1.5 * 2^30' | bc)"
            --cli-input-json "{ \"Metrics\": ${{ steps.prepared-metric-data-queries-json-inputs.outputs.total-memory }} }"
      - name: Run Sample App & OTel Collector + Load Generator
        working-directory: .github/app-collector-combo
        env:
          APP_IMAGE: ${{ steps.docker-image-metadata.outputs.tags }}
          INSTANCE_ID: ${{ github.run_id }}-${{ github.run_number }}
          LISTEN_ADDRESS: 0.0.0.0:8080
          LOG_GROUP_NAME: ${{ github.repository }}/soak-tests
          LOGS_NAMESPACE: ${{ github.repository }}/soak-tests
          LOG_STREAM_NAME: ${{ env.IMAGE_SUFFIX }}
          PROCESS_COMMAND_LINE: ${{ env.PROCESS_COMMAND_LINE }}
          PROCESS_EXECUTABLE_NAME: ${{ env.PROCESS_EXECUTABLE_NAME }}
          HOSTMETRICS_COLLECTION_INTERVAL_IN_SECONDS: ${{ env.HOSTMETRICS_COLLECTION_INTERVAL_IN_SECONDS }}
          SOAK_TEST_DURATION_IN_MINUTES: ${{ env.SOAK_TEST_DURATION_IN_MINUTES }}
        run: docker-compose up -d
      - name: Poll alarms during Soak Test
        continue-on-error: true
        id: check-failure-during-soak-tests
        run: >-
          pip install docker;
          python3 ./.github/scripts/poll-during-soak-tests.py --polling-interval ${{ env.HOSTMETRICS_COLLECTION_INTERVAL_IN_SECONDS }};
      - name: Get a snapshot of metrics
        run: >-
          mkdir -p soak-tests/snapshots/${{ github.sha }};
          for metric_kind in cpu-load total-memory;
          do
            aws cloudwatch get-metric-widget-image --metric-widget "$(
              cat ./.github/aws-cloudwatch-api-values/get-metric-widget-statistics/$metric_kind-metric-widget.json |
              sed -e "s/<SOAK_TEST_DURATION_IN_MINUTES>/${{ env.SOAK_TEST_DURATION_IN_MINUTES }}/g" \
                  -e "s/<METRIC_PERIOD>/${{ env.HOSTMETRICS_COLLECTION_INTERVAL_IN_SECONDS }}/g" \
                  -e "s/<NUM_OF_CPUS>/${{ steps.get-num-of-cpus-step.outputs.NUM_OF_CPUS }}/g" \
                  -e "s/<LOGS_NAMESPACE>/$(echo ${{ github.repository }}/soak-tests | sed 's/\//\\\//g')/g" \
                  -e "s/<PROCESS_COMMAND_LINE_DIMENSION_VALUE>/$(echo ${{ env.PROCESS_COMMAND_LINE }} | sed 's/\//\\\//g')/g"
            )" |
            jq -r '.MetricWidgetImage' |
            base64 -d > soak-tests/snapshots/${{ github.sha }}/${{ matrix.app-platform }}-${{ matrix.instrumentation-type }}-$metric_kind-results.png;
          done
      - name: Commit snapshots to repository
        uses: EndBug/add-and-commit@v7
        with:
          add: soak-tests/snapshots
          branch: gh-pages
          message: 'Adding Soak Tests Snapshots'
      - name: Move back to the main branch
        run: git checkout main
      - name: Prepare Soak Test results as JSON output
        run: >-
          declare -a COMMON_API_PARAMETERS=(
          --start-time $(date -u -d '${{ env.SOAK_TEST_DURATION_IN_MINUTES }} minutes ago' +%FT%TZ)
          --end-time $(date -u +%FT%TZ)
          );
          METRIC_DATA_QUERY_INPUT_JSON=$(
            jq '{ [ inputs ] | add }' ${{ steps.prepared-metric-data-queries-json-inputs.outputs.cpu-load }} ${{ steps.prepared-metric-data-queries-json-inputs.outputs.total-memory }}
          );
          aws cloudwatch get-metric-data "${COMMON_API_PARAMETERS[@]}" --cli-input-json "{ \"MetricDataQueries\": $METRIC_DATA_QUERY_INPUT_JSON" |
          jq "{
            benchmarks: [
              {
                Name: \"Soak Test Average CPU Load\",
                Value: first(
                    .MetricDataResults[] |
                    select(.Id == \"cpu_load_expr\")
                  ) |
                  .Values |
                  (
                    add /
                    length /
                    ${{ env.HOSTMETRICS_COLLECTION_INTERVAL_IN_SECONDS }} /
                    ${{ steps.get-num-of-cpus-step.outputs.NUM_OF_CPUS }} *
                    100
                  ),
                Unit: \"Percent\"
              },
              {
                Name: \"Soak Test Average Virtual Memory Used\",
                Value: first(
                    .MetricDataResults[] |
                    select(.Id == \"total_memory_expr\")
                  ) |
                  .Values |
                  (
                    add /
                    length /
                    $(echo '2^20' | bc)
                  ),
                Unit: \"Megabytes\"
              }
            ]
          }" |
          tee output.json
      - name: Report on Soak Test Averages results
        uses: NathanielRN/github-action-benchmark@v1.8.2-alpha3
        continue-on-error: true
        id: check-failure-after-soak-tests
        with:
          name: Soak Test Results - sample-app-${{ matrix.app-platform }}-${{ matrix.instrumentation-type }}
          tool: custombenchmark
          output-file-path: output.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          max-items-in-chart: 100
          alert-threshold: 1%
          # Does not work as expected, see: https://github.com/open-telemetry/opentelemetry-python/pull/1478
          # comment-always: true
          fail-on-alert: true
          auto-push: ${{ github.ref == 'refs/heads/main' }}
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: soak-tests/per-commit-overall-results
      - name: Publish Issue if failed DURING Soak Tests
        uses: JasonEtco/create-an-issue@v2
        if: steps.check-failure-after-soak-tests.outcome == 'failure'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          filename: .github/issue-templates/FAILURE-DURING-SOAK_TESTS.md
          update_existing: true
      - name: Publish Issue if failed AFTER Soak Tests
        uses: JasonEtco/create-an-issue@v2
        if: steps.check-failure-after-soak-tests.outcome == 'failure'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          filename: .github/issue-templates/FAILURE-AFTER-SOAK_TESTS.md
          update_existing: true
      - name: Check for Performance Degradation either DURING or AFTER Soak Tests
        if: steps.check-failure-after-soak-tests.outcome == 'failure' || steps.check-failure-after-soak-tests.outcome == 'failure'
        run: >-
          echo 'Soak Tests failed, see the logs above for details';
          exit 1;
      - name: Delete Soak Test alarms
        run: >-
          aws cloudwatch delete-alarms --alarm-names 'OTel Python Soak Tests - CPU Load Percentage Spike' 'OTel Python Soak Tests - Virtual Memory Usage Spike'
